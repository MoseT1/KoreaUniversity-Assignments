{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoseT1/KoreaUniversity-Assignments/blob/main/Lab02_MDP_(practice).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ytLdsw2pnaM"
      },
      "source": [
        "# Lab for Markov Decision Process\n",
        "1. GridWorld: a simple, finite-state MDP\n",
        "2. MDPs in Gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRr7JK01pnaN"
      },
      "source": [
        "### Install & import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT814Xj3pnaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ed26ea-cced-442e-ebaf-3702d08f2b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmAljFwrpnaO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "from gym.spaces import Discrete\n",
        "from IPython.display import clear_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Se_av44pnaO"
      },
      "source": [
        "# Recall: MDP is defined as follows. $\\cal{M} = <\\cal {S, A, P, R}, \\gamma>$\n",
        "- If we know the reward function $\\cal{R}$ and the transition probability $\\cal{P}$, we say we know the information about the environment, or the environment model is known. In that case, we can do planning -- we can obtain the best policy without having to interact with an environment.  \n",
        "\n",
        "- However in most real cases, those informations are unknown. If we don't know the environment, we can try a (environment) model-free approach which involves trial-and-errors.\n",
        "\n",
        "### Reinforcement Learning (RL) can do model-free learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igxgzyldpnaP"
      },
      "source": [
        "# MDP: GridWorld\n",
        "- This is a simple finite-state MDP you encountered in the previous lecture. Please read carefully, and understand how state, action, reward and transition probability is defined.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSFLCTxupnaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aab4fae-cffe-4919-ed5e-78702f0efcdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class Environment:\n",
        "    def __init__(self, env_config=None, *args, **kwargs) -> None:\n",
        "        self.grid_size = env_config['grid_size']\n",
        "        self.out_reward = env_config['out_reward']\n",
        "        self.step_reward = env_config['step_reward']\n",
        "        self.goal_reward = env_config['goal_reward']\n",
        "        self.max_step = env_config['max_step']\n",
        "        self.goal = env_config['goal']\n",
        "        self.action_space = Discrete(4)\n",
        "        self.observation_space = Discrete(self.grid_size*self.grid_size)\n",
        "        self.seeker = (0, 0)\n",
        "        self.info = {'seeker': self.seeker, 'goal': self.goal}\n",
        "        self.timestep = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.seeker = (0, 0) # row, col\n",
        "        self.timestep = 0\n",
        "        return self.get_observation()\n",
        "\n",
        "    def get_observation(self):\n",
        "        return self.grid_size * self.seeker[0] + self.seeker[1]\n",
        "\n",
        "    def get_reward(self):\n",
        "        return self.goal_reward if self.seeker == self.goal else 0\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.timestep == self.max_step:\n",
        "            return True\n",
        "        return self.seeker == self.goal\n",
        "\n",
        "    def check_pos(self, seeker):\n",
        "        is_out = False\n",
        "        if seeker[0] < 0 or seeker[0] > self.grid_size - 1 or \\\n",
        "            seeker[1] < 0 or seeker[1] > self.grid_size - 1:\n",
        "            is_out = True\n",
        "        return is_out\n",
        "\n",
        "    def step(self, action):\n",
        "        self.timestep += 1\n",
        "        reward = 0\n",
        "        is_out = False\n",
        "\n",
        "        if action == 0: # move left\n",
        "            self.seeker = (self.seeker[0], self.seeker[1] - 1)\n",
        "            is_out =  self.check_pos(self.seeker)\n",
        "            if is_out:\n",
        "                self.seeker = (self.seeker[0], self.seeker[1] + 1)\n",
        "\n",
        "        elif action == 1: # move right\n",
        "            self.seeker = (self.seeker[0], self.seeker[1] + 1)\n",
        "            is_out =  self.check_pos(self.seeker)\n",
        "            if is_out:\n",
        "                self.seeker = (self.seeker[0], self.seeker[1] - 1)\n",
        "\n",
        "        elif action == 2: # move up\n",
        "            self.seeker = (self.seeker[0] - 1, self.seeker[1])\n",
        "            is_out =  self.check_pos(self.seeker)\n",
        "            if is_out:\n",
        "                self.seeker = (self.seeker[0] + 1, self.seeker[1])\n",
        "\n",
        "        elif action == 3: # move down\n",
        "            self.seeker = (self.seeker[0] + 1, self.seeker[1])\n",
        "            is_out =  self.check_pos(self.seeker)\n",
        "            if is_out:\n",
        "                self.seeker = (self.seeker[0] - 1, self.seeker[1])\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "\n",
        "        if is_out:\n",
        "            reward = self.out_reward\n",
        "        else:\n",
        "            reward = self.get_reward() + self.step_reward\n",
        "\n",
        "        return self.get_observation(), reward, self.is_done(), self.info\n",
        "\n",
        "    def render(self, *args, **kwaargs):\n",
        "        os.system('cls' if os.name == 'nt' else 'clear')\n",
        "        clear_output(wait=True)\n",
        "        grid_row = ['| ' for _ in range(self.grid_size)]\n",
        "        grid = [grid_row + [\"|\\n\"] for _ in range(self.grid_size)]\n",
        "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
        "        grid[self.seeker[0]][self.seeker[1]] = '|A'\n",
        "        print(''.join([''.join(grid_row) for grid_row in grid]))\n",
        "\n",
        "\n",
        "class Gridworld(Environment, gym.Env):\n",
        "    def __init__(self, env_config, *args, **kwargs) -> None:\n",
        "        super().__init__(env_config=env_config, *args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Gp60MCpnaP"
      },
      "source": [
        "By recognizing the environment, you can find out that it has 4 actions:\n",
        "\n",
        "0: LEFT  \n",
        "1: RIGHT  \n",
        "2: UP   \n",
        "3: DOWN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4nUlNwc3fXd"
      },
      "outputs": [],
      "source": [
        "from enum import IntEnum\n",
        "\n",
        "class Action(IntEnum):\n",
        "    LEFT = 0\n",
        "    RIGHT = 1\n",
        "    UP = 2\n",
        "    DOWN = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzTv-i3_3fjc"
      },
      "source": [
        "Note that you can change the grid size, goal reward, and max_step of the environment. Let's start from simple setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO9meX0JpnaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ae374d-d897-49f3-fd70-048ff3bf4edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|A| | | |\n",
            "| | | | |\n",
            "| | | | |\n",
            "| | | |G|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env_config = {\"grid_size\": 4,\n",
        "              \"goal\": (3, 3),\n",
        "             \"goal_reward\": 1,\n",
        "             \"step_reward\": -1,\n",
        "             \"out_reward\": -5,\n",
        "             \"max_step\": 20 }\n",
        "env = Gridworld(env_config)\n",
        "# render shows the grid map\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "NkualeFpLC8L",
        "outputId": "3e84d3ca-45a7-459e-ca58-fa0ae334573f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-20-e852b39f3303>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-e852b39f3303>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(env.seeker\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMeNGCKG7DId"
      },
      "source": [
        "Check what happens after applying action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx6__GyBpnaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e1d335-599a-4dc0-9feb-6e8f6948c709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|A| | | |\n",
            "| | | | |\n",
            "| | | | |\n",
            "| | | |G|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.step(0)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9mimEH7pnaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ac83a7-0723-433b-c21d-173767b36c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | | |\n",
            "|A| | | |\n",
            "| | | | |\n",
            "| | | |G|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.step(3)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsGwCnjpnaQ"
      },
      "source": [
        "## 1. Random policy  \n",
        "\n",
        "We learned that we need a policy $\\pi$ to solve MDP. Let's first try random policy, and see how it performs.\n",
        "\n",
        "## TODO 1. Implement the random policy\n",
        "- implement the random_policy function\n",
        "  - Input: env\n",
        "  - Return: random action (0~3)\n",
        "\n",
        "### Hints\n",
        "- Utilize the `random` built-in library` and action information\n",
        "  - There is a built-in function `random.randint(min, max)`\n",
        "  - You can get the size of action dimension by using `env.action_space.n`\n",
        "- You can also use the `env.action_space.sample()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIqU8FeRpnaQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_policy(env):\n",
        "    ### Implement here\n",
        "    if env.action_space.n <= env.max_step:\n",
        "      action = random.randint(0,3)\n",
        "      return action\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-rZHBK6pnaQ"
      },
      "source": [
        "# Test your random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijNQ1splpnaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991af181-62dd-4e9a-87e4-52626a0f27c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | | |\n",
            "| | | | |\n",
            "| | | | |\n",
            "| | | |A|\n",
            "\n",
            "Action.DOWN\n",
            "current episode: 1\n",
            "steps: 10\n",
            "agent pos. x:  3, y:  3\n",
            "goal pos.  x:  3, y:  3\n",
            "total reward: -17\n"
          ]
        }
      ],
      "source": [
        "render_time = 0.5\n",
        "render = True\n",
        "for episode in range(0, 2):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    current_step = 0\n",
        "    while not done:\n",
        "        action = random_policy(env)\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        current_step += 1\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        if render:\n",
        "            time.sleep(render_time)\n",
        "            env.render()\n",
        "            print(Action(action))\n",
        "            print(f\"current episode: {episode}\")\n",
        "            print(f\"steps: {current_step}\")\n",
        "            print(f\"agent pos. x: {env.seeker[0]:>2d}, y: {env.seeker[1]:>2d}\")\n",
        "            print(f\"goal pos.  x: {env.goal[0]:>2d}, y: {env.goal[1]:>2d}\")\n",
        "            print(f\"total reward: {total_reward:>2d}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRNBzw3-qkJH",
        "outputId": "d9191fd9-0dce-4646-eba7-48320a9fe368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n58KwcOGpnaQ"
      },
      "source": [
        "## 2. Manual policy   \n",
        "\n",
        "Did you see the performance of random policy? It fails to reach the destination in most cases.  Let's implement manual policy!\n",
        "\n",
        "## TODO 2. Implement the manual policy\n",
        "- Implement the manual_policy function\n",
        "  - Input: state\n",
        "  - Return: appropriate action\n",
        "  \n",
        "## Hints\n",
        "- Utilize the `if, elif, else` conditional statement and enum class `Action`\n",
        "- Utilize `random.choice(list)` built-in function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt4Kib5hpnaR",
        "outputId": "fb2c483e-e2f2-452b-8f31-a1bfbe47b07c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "# example) random.choice\n",
        "action_list = [1,3]\n",
        "action = random.choice(action_list)\n",
        "print(action)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2rWv8joQ1oR",
        "outputId": "3c4b83e3-a049-4ff7-ddbf-965123c60b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<enum 'Action'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUhwNBAipnaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e3d4ad-f3aa-4b2e-d4b0-709b98b359b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def manual_policy(env):\n",
        "    ### Implement here ###\n",
        "    if state in [0,1,2,4,5,6,8,9,10]:\n",
        "      action = random.choice(action_list)\n",
        "    elif state in [3,7,11]:\n",
        "      action = 3\n",
        "    else:\n",
        "      action =1\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l-kjasVpnaU"
      },
      "source": [
        "# Test your manual policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtxJZ649pnaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2451c5f6-ec13-407f-97c9-b38672f95ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | | |\n",
            "| | | | |\n",
            "| | | | |\n",
            "| | | |A|\n",
            "\n",
            "Action.RIGHT\n",
            "current episode: 9\n",
            "steps: 6\n",
            "agent pos. x:  3, y:  3\n",
            "goal pos.  x:  3, y:  3\n",
            "total reward: -5\n"
          ]
        }
      ],
      "source": [
        "render_time = 0.5 # this must be set large enough so that colab do not skip the printouts\n",
        "render = True\n",
        "for episode in range(0, 10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    current_step = 0\n",
        "    while not done:\n",
        "        action = manual_policy(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        current_step += 1\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        if render:\n",
        "            time.sleep(render_time)\n",
        "            env.render()\n",
        "            print(Action(action))\n",
        "            print(f\"current episode: {episode}\")\n",
        "            print(f\"steps: {current_step}\")\n",
        "            print(f\"agent pos. x: {env.seeker[0]:>2d}, y: {env.seeker[1]:>2d}\")\n",
        "            print(f\"goal pos.  x: {env.goal[0]:>2d}, y: {env.goal[1]:>2d}\")\n",
        "            print(f\"total reward: {total_reward:>2d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh5HuS4OpnaV"
      },
      "source": [
        "# Well done!\n",
        "- Now, how about this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckkiocQ-pnaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c629f54-8d1c-41c4-ca74-e19839370764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|A| | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "|G| | | | | | | |\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env_config = {\"grid_size\": 8,\n",
        "              \"goal\": (7, 0),\n",
        "             \"goal_reward\": 1,\n",
        "             \"step_reward\": -1,\n",
        "             \"out_reward\": -5,\n",
        "             \"max_step\": 50 }\n",
        "env = Gridworld(env_config)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82srYDRuop7C"
      },
      "source": [
        "and this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDiGa_pwpnaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89cd953c-4577-4bf1-f8ce-4ed2b1e495a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|A| | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | |G| | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "| | | | | | | | | | | | | | | | |\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env_config = {\"grid_size\": 16,\n",
        "              \"goal\": (13, 11),\n",
        "             \"goal_reward\": 1,\n",
        "             \"step_reward\": -1,\n",
        "             \"out_reward\": -5,\n",
        "             \"max_step\": 250 }\n",
        "env = Gridworld(env_config)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g928tnCRpnaV"
      },
      "source": [
        "# Of course, you can implement manual policies for those environments.\n",
        "- Is it meaningful?\n",
        "- what happens if it goes high-dimensional, or more complex, and ...\n",
        "\n",
        "### Resources are limited - Instead of designing policies manually, we can make computer to learn the policy.\n",
        "### That's what reinforcement learning is about!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkLJs8gKpnaV"
      },
      "source": [
        "# 3. RL policy\n",
        "\n",
        "We will see how reinforcement learning works here. This algorithm is called Q-learning, and is not yet appeared in the lecture (so don't be stressed if it is difficult to understand!)\n",
        "\n",
        "### The policy is provoided as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V__Y90RIpnaV"
      },
      "outputs": [],
      "source": [
        "class Policy:\n",
        "    def __init__(self, env): # Q-table of size |S| x |A|\n",
        "        self.state_action_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "        self.action_space = env.action_space\n",
        "\n",
        "    def get_action(self, state, explore=True, epsilon=0.1): # epsilon-greedy\n",
        "        if explore and random.uniform(0, 1) < epsilon:\n",
        "            return self.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.state_action_table[state, :])\n",
        "\n",
        "    def save(self, num):\n",
        "        name = f\"policy{num}.npy\"\n",
        "        np.save(name, self.state_action_table)\n",
        "\n",
        "    def load(self, npy_path):\n",
        "         self.state_action_table = np.load(npy_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guO_DycNpnaV"
      },
      "source": [
        "# Rollout function\n",
        "\n",
        "In reinforcement learning, `rollout` refers to the process of getting samples (=experiences) via agent-environment interactions.\n",
        "- Inputs: policy, environment, maximum length of episode (T), ... etc\n",
        "- Return: Trajectory (experiences, or (s, a, s', r, d) x ?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWY0BAhXpnaV"
      },
      "outputs": [],
      "source": [
        "def rollout(env, policy, T, *args, **kwargs):\n",
        "    render = kwargs[\"render\"]\n",
        "    epsilon = kwargs[\"epsilon\"]\n",
        "    render_time = kwargs[\"render_time\"]\n",
        "    explore = kwargs[\"explore\"]\n",
        "    episode = kwargs[\"episode\"]\n",
        "\n",
        "    experiences = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    current_step = 0\n",
        "    while not done:\n",
        "        action = policy.get_action(state, explore, epsilon)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        experiences.append([state, action, reward, next_state, done])\n",
        "        current_step += 1\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        if render:\n",
        "            time.sleep(render_time)\n",
        "            env.render()\n",
        "            print(Action(action))\n",
        "            print(f\"current episode: {episode}\")\n",
        "            print(f\"current step: {current_step}\")\n",
        "            print(f\"agent pos. x: {env.seeker[0]:>2d}, y: {env.seeker[1]:>2d}\")\n",
        "            print(f\"goal pos.  x: {env.goal[0]:>2d}, y: {env.goal[1]:>2d}\")\n",
        "            print(f\"total reward: {total_reward:>2d}\")\n",
        "\n",
        "        if current_step == T:\n",
        "            break\n",
        "    return experiences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV0Mkj7DpnaV"
      },
      "source": [
        "# Update functions\n",
        "These functions are used to train and evaluate the Q-learning agent.\n",
        "- update_policy: Update the policy by using Q-learning equation\n",
        "- train_policy: Iterate update_policy over the episodes.\n",
        "- evaluate_policy: Evaluate and visualize to see if the learning went well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5zc4whwpnaV"
      },
      "outputs": [],
      "source": [
        "def update_policy(policy, trajectory, weight=0.1, discount_factor=0.9):\n",
        "    random.shuffle(trajectory)\n",
        "    for state, action, reward, next_state, done in trajectory:\n",
        "        next_max = np.max(policy.state_action_table[next_state, :])\n",
        "        value = policy.state_action_table[state, action]\n",
        "        new_value = (1 - weight) * value + weight * (reward + discount_factor * next_max)\n",
        "        policy.state_action_table[state][action] = new_value\n",
        "\n",
        "def train_policy(env, policy, T=20, num_episodes=10000, weight=0.1, discount_factor=0.9, epsilon=0.5):\n",
        "    for e in range(1, num_episodes+1):\n",
        "        trajectory = rollout(env, policy, T, render=False, render_time=0.0, explore=True, epsilon=epsilon, episode=e)\n",
        "        update_policy(policy, trajectory, weight, discount_factor)\n",
        "\n",
        "    policy.save(e)\n",
        "    return policy\n",
        "\n",
        "def evaluate_policy(env, policy, T, npy_path, num_episodes=5):\n",
        "    policy.load(npy_path)\n",
        "    steps = 0\n",
        "    total_reward_lst = []\n",
        "    avg_score = 0\n",
        "    for e in range(num_episodes):\n",
        "        experiences = rollout(env, policy, T, render=True, render_time=0.1, explore=False, epsilon=0, episode=e)\n",
        "        total_reward = 0\n",
        "        for transition in experiences:\n",
        "            total_reward += transition[2]\n",
        "        total_reward_lst.append(total_reward)\n",
        "        steps += len(experiences)\n",
        "\n",
        "    avg_score = sum(total_reward_lst) / len(total_reward_lst)\n",
        "    return steps / num_episodes, avg_score, total_reward_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq9_B4vFpnaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37da6785-0fd4-459e-976b-da5408482255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|A| | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | |G| |\n",
            "| | | | | | | | |\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env_config = {\"grid_size\": 8,\n",
        "              \"goal\": (6, 6),\n",
        "             \"goal_reward\": 1,\n",
        "             \"step_reward\": -1,\n",
        "             \"out_reward\": -5,\n",
        "             \"max_step\": 40 }\n",
        "env = Gridworld(env_config)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk-u4Nym2G_3"
      },
      "source": [
        "# Training a policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT2LGzZ4pnaV"
      },
      "outputs": [],
      "source": [
        "untrained_policy = Policy(env)\n",
        "trained_policy = train_policy(env, untrained_policy, env_config[\"max_step\"], num_episodes=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDYREvsk2Mjg"
      },
      "source": [
        "# Evaluate trained policy\n",
        "- Check the Action-Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KryCwBWK2QjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772a92b0-649c-4350-e99b-32af68e46a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action-Values:  [(<Action.LEFT: 0>, -11.18), (<Action.RIGHT: 1>, -6.86), (<Action.UP: 2>, -11.18), (<Action.DOWN: 3>, -6.86)]\n",
            "Action-Values:  [(<Action.LEFT: 0>, -1.88), (<Action.RIGHT: 1>, -1.87), (<Action.UP: 2>, 0.0), (<Action.DOWN: 3>, -4.89)]\n"
          ]
        }
      ],
      "source": [
        "# check action-values at state 0\n",
        "print('Action-Values: ', [(Action(i), a) for i, a in enumerate(np.round(trained_policy.state_action_table[0, :], 2))])\n",
        "# check action-values at state 14\n",
        "print('Action-Values: ', [(Action(i), a) for i, a in enumerate(np.round(trained_policy.state_action_table[-2, :], 2))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1FJzdIE4kMB"
      },
      "source": [
        "- Check the performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekb-b7uxpnaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69b0434-2518-4609-e095-62b757435bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | | | |\n",
            "| | | | | | |A| |\n",
            "| | | | | | | | |\n",
            "\n",
            "Action.DOWN\n",
            "current episode: 4\n",
            "current step: 12\n",
            "agent pos. x:  6, y:  6\n",
            "goal pos.  x:  6, y:  6\n",
            "total reward: -11\n"
          ]
        }
      ],
      "source": [
        "avg_steps, avg_score, total_reward_lst = evaluate_policy(env, trained_policy, env_config[\"max_step\"], \"policy10000.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcfjuZG3pnaW",
        "outputId": "dd52648f-53ae-43c5-d13b-5edca450aab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg_steps: 12.0, avg_score: -11.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"avg_steps: {avg_steps}, avg_score: {avg_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEHcAJbUpnaO"
      },
      "source": [
        "# Gymnasium provides various MDPs!\n",
        "- You can check their configurations as well\n",
        "- If you are more curious, you can see how they are implemented; Gymnasium is open-source library!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YobGBMMcpnaW",
        "outputId": "d528049a-1ff7-48b5-b05e-6297e42d099d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(16)\n",
            "Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "# toy text - Frozenlake\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# state (observation) and action space information\n",
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjaueNKNpnaP"
      },
      "outputs": [],
      "source": [
        "env2 = gym.make('CartPole-v1')\n",
        "### print observation_space and action_space\n",
        "\n",
        "env3 = gym.make('Pendulum-v1')\n",
        "### print observation_space and action_space\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NaGQNBn8Y16"
      },
      "source": [
        "# Get used to Gymnasium\n",
        "- will use it throughout the course."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c6276770e9c5a685155d297a75ee9fe2216c271da1ff18b89edbaa80fe1643b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
